{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/yjenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 11:36:03.408845: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-28 11:36:03.422993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743132963.439145 1837107 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743132963.443913 1837107 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743132963.456813 1837107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743132963.456832 1837107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743132963.456834 1837107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743132963.456835 1837107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-28 11:36:03.461276: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:09<00:00,  1.99s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor,Qwen2VLForConditionalGeneration\n",
    "# from transformers import AutoProcessor,Qwen2VLForConditionalGeneration\n",
    "import numpy as np\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "local_path = \"/cpfs04/shared/CausaLLMs/HuggingfaceModels/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    local_path, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# local_path = \"/cpfs04/shared/CausaLLMs/HuggingfaceModels/QVQ-72B-Preview\"\n",
    "\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     local_path, torch_dtype=\"auto\", device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def evaluate_image(image_path,image_size) -> str:\n",
    "    \"\"\"\n",
    "    Evaluates the given image for internal visual consistency and returns a score.\n",
    "    \n",
    "    The evaluation is performed by prompting the model with a message asking to rate the image\n",
    "    from 0 (inconsistent) to 10 (completely harmonious and consistent), following the format:\n",
    "    \"The score is {your_score}. The reason is {your_reason}.\"\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): The path to the image.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated output text containing the score and the reason.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path,\"resized_height\":image_size,\"resized_width\":image_size},\n",
    "                {\"type\": \"text\", \"text\": (\n",
    "                    \"You're an expert in understanding images. Each image can be divided into 4 numbers along the horizontal and vertical midlines. First, identify these 4 numbers separately, calculate their sum, and check if it's 24. If yes, label it as 1; otherwise, label it as 0.\"\n",
    "                    \"Please think this step by step and answer in the the format: The label is {your_label}. The reason is {your_reason}.\"\n",
    "                )},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=1000)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    # 返回生成的评分信息（例如字符串形式）\n",
    "\n",
    "    # print(\"image_path:\",image_path,\"output_text:\",output_text)\n",
    "    # return extract_score_after_final_answer(output_text) \n",
    "    pattern = r\"(?i)(?:The\\s+label is)[\\s:-]*(\\d+)\"\n",
    "    match = re.search(pattern, output_text[0])\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        raise ValueError(f\"Score not found in output: {output_text[0]}\")\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_score_after_final_answer(output_text):\n",
    "    \"\"\"\n",
    "    从给定文本中查找 **Final Answer** 后面出现的第一个数字，并返回其整数值。\n",
    "    如果未找到，则抛出 ValueError。\n",
    "    \"\"\"\n",
    "    # 若 output_text 是列表，则默认取第一个元素\n",
    "    if isinstance(output_text, list):\n",
    "        text_to_search = output_text[0]\n",
    "    else:\n",
    "        text_to_search = output_text\n",
    "\n",
    "    # 正则：匹配 \"**Final Answer**\" 后，任意字符(含换行)，再遇到数字(\\d+)\n",
    "    pattern = r\"\\*\\*Final Answer\\*\\*.*?(\\d+)\"\n",
    "    match = re.search(pattern, text_to_search, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        raise ValueError(f\"No number found after **Final Answer** in: {text_to_search}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_label(text):\n",
    "    # 增强版正则表达式，处理多种文本格式\n",
    "    pattern = r\"(?i)(?:The\\s+label is)[\\s:-]*(\\d+)\"\n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        # 调试模式：显示所有潜在匹配\n",
    "        debug_matches = re.findall(r'\\d+', text)\n",
    "        raise ValueError(f\"未找到标签值，文本中所有数字：{debug_matches}\")\n",
    "\n",
    "# 测试用例\n",
    "test_text = \"Therefore, the label is 1 because the sum of the numbers is 24. The label is 1. The reason is the sum of the numbers 2, 8, 7, and 7 is 24.\"\n",
    "print(extract_label(test_text))  # 输出: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "image_dir = \"/cpfs04/user/hanyujin/rule-gen/datasets/mnist-mnist/images\"\n",
    "image_dir_1 = \"/cpfs04/user/hanyujin/causal-dm/results/mnist-mnist_unet/vis/epoch_500_1743129768.0850055\"\n",
    "image_dir_2 = \"/cpfs04/user/hanyujin/causal-dm/AR_diff/results/mnist-mnist_parallelFalse_reverseFalse_patch32_dim480_depth6_mlpdepth3_noweightedlossTrue_trainsteps125000_mlp/generations/checkpoint.125000\"\n",
    "# 获取所有 PNG 文件\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith(\".png\")]\n",
    "image_files_1 = [f for f in os.listdir(image_dir_1) if f.endswith(\".png\")]\n",
    "image_files_2 = [f for f in os.listdir(image_dir_2) if f.endswith(\".png\")]\n",
    "# 随机采样100张图片（如果图片总数不足，则全部使用）\n",
    "sample_size = min(10, len(image_files))\n",
    "sampled_files = random.sample(image_files, sample_size) #sorted(image_files)[:sample_size] #random.sample(image_files_1, sample_size)\n",
    "\n",
    "sample_size = min(10, len(image_files_1))\n",
    "sampled_files_1 = random.sample(image_files_1, sample_size) #sorted(image_files_1)[:sample_size] #random.sample(image_files_1, sample_size)\n",
    "\n",
    "sample_size = min(10, len(image_files_2))\n",
    "sampled_files_2 = random.sample(image_files_2, sample_size) #sorted(image_files_2)[:sample_size] #random.sample(image_files_2, sample_size)\n",
    "\n",
    "image_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raw Images:  10%|█         | 1/10 [00:05<00:46,  5.13s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image combined_0168.png: Score not found in output: Let's break down the image into its four numbers:\n",
      "\n",
      "1. The number at the top-left corner is \\(0\\).\n",
      "2. The number at the top-right corner is \\(7\\).\n",
      "3. The number at the bottom-left corner is \\(9\\).\n",
      "4. The number at the bottom-right corner is \\(8\\).\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[0 + 7 + 9 + 8 = 24\\]\n",
      "\n",
      "Since the sum of these numbers is indeed 24, we can conclude that the label should be 1.\n",
      "\n",
      "The label is \\(\\boxed{1}\\). The reason is that the sum of the numbers \\(0 + 7 + 9 + 8\\) equals 24.\n",
      "Train image scores: [None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raw Images:  20%|██        | 2/10 [00:09<00:37,  4.68s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image combined_1857.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. Top left number: 4\n",
      "2. Top right number: 7\n",
      "3. Bottom left number: 2\n",
      "4. Bottom right number: 8\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 4 + 7 + 2 + 8 = 21 \\]\n",
      "\n",
      "The sum of these numbers is 21, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (4 + 7 + 2 + 8) is 21, not 24.\n",
      "Train image scores: [None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raw Images:  30%|███       | 3/10 [00:13<00:31,  4.51s/it, avg_score=1.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image scores: [None, None, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raw Images:  40%|████      | 4/10 [00:18<00:27,  4.60s/it, avg_score=1.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image scores: [None, None, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raw Images:  50%|█████     | 5/10 [00:22<00:22,  4.47s/it, avg_score=1.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image scores: [None, None, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raw Images:  60%|██████    | 6/10 [00:27<00:17,  4.40s/it, avg_score=1.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image scores: [None, None, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raw Images:  70%|███████   | 7/10 [00:31<00:13,  4.45s/it, avg_score=1.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image combined_0110.png: Score not found in output: Let's break down the image into the four numbers:\n",
      "\n",
      "1. Top left number: 6\n",
      "2. Top right number: 3\n",
      "3. Bottom left number: 4\n",
      "4. Bottom right number: 9\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 6 + 3 + 4 + 9 = 22 \\]\n",
      "\n",
      "The sum of the four numbers is 22, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the four numbers (6 + 3 + 4 + 9) is 22, not 24.\n",
      "Train image scores: [None, None, 1, 1, 1, 1, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raw Images:  80%|████████  | 8/10 [00:36<00:08,  4.44s/it, avg_score=1.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image combined_0210.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. Top left number: 63\n",
      "2. Top right number: 9\n",
      "3. Bottom left number: 6\n",
      "4. Bottom right number: 6\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 63 + 9 + 6 + 6 = 84 \\]\n",
      "\n",
      "The sum of these numbers is 84, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (63, 9, 6, 6) is 84, not 24.\n",
      "Train image scores: [None, None, 1, 1, 1, 1, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raw Images:  90%|█████████ | 9/10 [00:40<00:04,  4.54s/it, avg_score=1.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image combined_0969.png: Score not found in output: Let's break down the image into its four numbers:\n",
      "\n",
      "1. The number at the top left is 7.\n",
      "2. The number at the top right is 7.\n",
      "3. The number at the bottom left is 7.\n",
      "4. The number at the bottom right is 8.\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 7 + 7 + 7 + 8 = 29 \\]\n",
      "\n",
      "The sum of the four numbers is 29, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the four numbers (7 + 7 + 7 + 8) is 29, not 24.\n",
      "Train image scores: [None, None, 1, 1, 1, 1, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raw Images: 100%|██████████| 10/10 [00:45<00:00,  4.51s/it, avg_score=1.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image combined_1109.png: Score not found in output: Let's break down the image into its four numbers:\n",
      "\n",
      "1. Top left number: 8\n",
      "2. Top right number: 6\n",
      "3. Bottom left number: 2\n",
      "4. Bottom right number: 9\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 8 + 6 + 2 + 9 = 25 \\]\n",
      "\n",
      "The sum of the four numbers is 25, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the four numbers (8, 6, 2, 9) is 25, not 24.\n",
      "Train image scores: [None, None, 1, 1, 1, 1, None, None, None, None]\n",
      "Train image scores: [None, None, 1, 1, 1, 1, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# image_size = 64\n",
    "scores_original = []\n",
    "running_sum = 0\n",
    "valid_count = 0\n",
    "pbar = tqdm(sampled_files, desc=\"Processing Raw Images\")\n",
    "for image_name in pbar:\n",
    "    image_path = os.path.join(image_dir, image_name)\n",
    "    try:\n",
    "        score = evaluate_image(image_path,image_size)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating original image {image_name}: {e}\")\n",
    "        score = None\n",
    "\n",
    "    if score is not None:\n",
    "        running_sum += score\n",
    "        valid_count += 1\n",
    "\n",
    "    scores_original.append(score)\n",
    "    print(\"Train image scores:\", scores_original)\n",
    "    avg_score = running_sum / valid_count if valid_count > 0 else 0\n",
    "    pbar.set_postfix(avg_score=f\"{avg_score:.4f}\")\n",
    "print(\"Train image scores:\", scores_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  10%|█         | 1/10 [00:04<00:44,  4.95s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_188.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. The number on the top left appears to be 2.\n",
      "2. The number on the top right appears to be 2.\n",
      "3. The number on the bottom left appears to be 4.\n",
      "4. The number on the bottom right appears to be 4.\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 2 + 2 + 4 + 4 = 12 \\]\n",
      "\n",
      "The sum of these numbers is 12, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (2, 2, 4, 4) is 12, not 24.\n",
      "DDPM image scores: [None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  20%|██        | 2/10 [00:09<00:36,  4.61s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_59.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. Top left number: 9\n",
      "2. Top right number: 8\n",
      "3. Bottom left number: 6\n",
      "4. Bottom right number: 5\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 9 + 8 + 6 + 5 = 28 \\]\n",
      "\n",
      "The sum of the four numbers is 28, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the four numbers (9 + 8 + 6 + 5) is 28, not 24.\n",
      "DDPM image scores: [None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  30%|███       | 3/10 [00:13<00:31,  4.47s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_105.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. Top left number: 6\n",
      "2. Top right number: 2\n",
      "3. Bottom left number: 9\n",
      "4. Bottom right number: 3\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 6 + 2 + 9 + 3 = 20 \\]\n",
      "\n",
      "The sum of the numbers is 20, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (6 + 2 + 9 + 3) is 20, not 24.\n",
      "DDPM image scores: [None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  40%|████      | 4/10 [00:17<00:26,  4.41s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_159.png: Score not found in output: Let's break down the image into its four numbers:\n",
      "\n",
      "1. Top-left number: 1\n",
      "2. Top-right number: 8\n",
      "3. Bottom-left number: 9\n",
      "4. Bottom-right number: 8\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 1 + 8 + 9 + 8 = 26 \\]\n",
      "\n",
      "The sum of the four numbers is 26, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the four numbers (1 + 8 + 9 + 8) is 26, not 24.\n",
      "DDPM image scores: [None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  50%|█████     | 5/10 [00:22<00:21,  4.40s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_135.png: Score not found in output: Let's break down the image into the four numbers:\n",
      "\n",
      "1. Top left number: 5\n",
      "2. Top right number: 4\n",
      "3. Bottom left number: 9\n",
      "4. Bottom right number: 9\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 5 + 4 + 9 + 9 = 27 \\]\n",
      "\n",
      "The sum of the four numbers is 27, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (5 + 4 + 9 + 9) is 27, not 24.\n",
      "DDPM image scores: [None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  60%|██████    | 6/10 [00:27<00:18,  4.58s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_150.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. The number at the top left appears to be '5'.\n",
      "2. The number at the top right appears to be '9'.\n",
      "3. The number at the bottom left appears to be '8'.\n",
      "4. The number at the bottom right appears to be '4'.\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 5 + 9 + 8 + 4 = 26 \\]\n",
      "\n",
      "The sum of these numbers is 26, not 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (5, 9, 8, 4) is 26, which does not equal 24.\n",
      "DDPM image scores: [None, None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  70%|███████   | 7/10 [00:31<00:13,  4.63s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_56.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. The top left number appears to be 9.\n",
      "2. The top right number appears to be 6.\n",
      "3. The bottom left number appears to be 9.\n",
      "4. The bottom right number appears to be 7.\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 9 + 6 + 9 + 7 = 31 \\]\n",
      "\n",
      "The sum of the numbers is 31, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (9 + 6 + 9 + 7) is 31, not 24.\n",
      "DDPM image scores: [None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  80%|████████  | 8/10 [00:36<00:09,  4.55s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_109.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. Top left number: 6\n",
      "2. Top right number: 8\n",
      "3. Bottom left number: 9\n",
      "4. Bottom right number: 7\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 6 + 8 + 9 + 7 = 30 \\]\n",
      "\n",
      "The sum of the four numbers is 30, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the four numbers (6 + 8 + 9 + 7) is 30, not 24.\n",
      "DDPM image scores: [None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  90%|█████████ | 9/10 [00:40<00:04,  4.49s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_158.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. Top-left number: 2\n",
      "2. Top-right number: 4\n",
      "3. Bottom-left number: 6\n",
      "4. Bottom-right number: 6\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 2 + 4 + 6 + 6 = 18 \\]\n",
      "\n",
      "The sum of the numbers is 18, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (2 + 4 + 6 + 6) is 18, not 24.\n",
      "DDPM image scores: [None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images: 100%|██████████| 10/10 [00:45<00:00,  4.55s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_160.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. The top-left number appears to be 9.\n",
      "2. The top-right number appears to be 5.\n",
      "3. The bottom-left number appears to be 6.\n",
      "4. The bottom-right number appears to be 8.\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 9 + 5 + 6 + 8 = 28 \\]\n",
      "\n",
      "The sum of the numbers is 28, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (9 + 5 + 6 + 8) is 28, not 24.\n",
      "DDPM image scores: [None, None, None, None, None, None, None, None, None, None]\n",
      "DDPM image scores: [None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 用于存储 Qwen 得分\n",
    "scores_original_1 = []\n",
    "running_sum = 0\n",
    "valid_count = 0\n",
    "pbar = tqdm(sampled_files_1, desc=\"Processing DDPM Images\")\n",
    "for image_name in pbar:\n",
    "    image_path = os.path.join(image_dir_1, image_name)\n",
    "    try:\n",
    "        score = evaluate_image(image_path,image_size)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating original image {image_name}: {e}\")\n",
    "        score = None\n",
    "\n",
    "    if score is not None:\n",
    "        running_sum += score\n",
    "        valid_count += 1\n",
    "\n",
    "    scores_original_1.append(score)\n",
    "    print(\"DDPM image scores:\", scores_original_1)\n",
    "    avg_score = running_sum / valid_count if valid_count > 0 else 0\n",
    "    pbar.set_postfix(avg_score=f\"{avg_score:.4f}\")\n",
    "print(\"DDPM image scores:\", scores_original_1)\n",
    "# [2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  10%|█         | 1/10 [00:04<00:44,  4.93s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_188.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. The number on the top left appears to be 2.\n",
      "2. The number on the top right appears to be 2.\n",
      "3. The number on the bottom left appears to be 4.\n",
      "4. The number on the bottom right appears to be 4.\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 2 + 2 + 4 + 4 = 12 \\]\n",
      "\n",
      "The sum of these numbers is 12, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (2, 2, 4, 4) is 12, not 24.\n",
      "AR image scores: [None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  20%|██        | 2/10 [00:09<00:38,  4.75s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_59.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. Top left number: 9\n",
      "2. Top right number: 8\n",
      "3. Bottom left number: 6\n",
      "4. Bottom right number: 5\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 9 + 8 + 6 + 5 = 28 \\]\n",
      "\n",
      "The sum of the four numbers is 28, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the four numbers (9 + 8 + 6 + 5) is 28, not 24.\n",
      "AR image scores: [None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  30%|███       | 3/10 [00:13<00:31,  4.53s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_105.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. Top left number: 6\n",
      "2. Top right number: 2\n",
      "3. Bottom left number: 9\n",
      "4. Bottom right number: 3\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 6 + 2 + 9 + 3 = 20 \\]\n",
      "\n",
      "The sum of the numbers is 20, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (6 + 2 + 9 + 3) is 20, not 24.\n",
      "AR image scores: [None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  40%|████      | 4/10 [00:18<00:27,  4.56s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_159.png: Score not found in output: Let's break down the image into its four numbers:\n",
      "\n",
      "1. Top-left number: 1\n",
      "2. Top-right number: 8\n",
      "3. Bottom-left number: 9\n",
      "4. Bottom-right number: 8\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 1 + 8 + 9 + 8 = 26 \\]\n",
      "\n",
      "The sum of the four numbers is 26, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the four numbers (1 + 8 + 9 + 8) is 26, not 24.\n",
      "AR image scores: [None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  50%|█████     | 5/10 [00:22<00:22,  4.54s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_135.png: Score not found in output: Let's break down the image into the four numbers:\n",
      "\n",
      "1. Top left number: 5\n",
      "2. Top right number: 4\n",
      "3. Bottom left number: 9\n",
      "4. Bottom right number: 9\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 5 + 4 + 9 + 9 = 27 \\]\n",
      "\n",
      "The sum of the four numbers is 27, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (5 + 4 + 9 + 9) is 27, not 24.\n",
      "AR image scores: [None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  60%|██████    | 6/10 [00:27<00:18,  4.70s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_150.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. The number at the top left appears to be '5'.\n",
      "2. The number at the top right appears to be '9'.\n",
      "3. The number at the bottom left appears to be '8'.\n",
      "4. The number at the bottom right appears to be '4'.\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 5 + 9 + 8 + 4 = 26 \\]\n",
      "\n",
      "The sum of these numbers is 26, not 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (5, 9, 8, 4) is 26, which does not equal 24.\n",
      "AR image scores: [None, None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  70%|███████   | 7/10 [00:32<00:14,  4.70s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_56.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. The top left number appears to be 9.\n",
      "2. The top right number appears to be 6.\n",
      "3. The bottom left number appears to be 9.\n",
      "4. The bottom right number appears to be 7.\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 9 + 6 + 9 + 7 = 31 \\]\n",
      "\n",
      "The sum of the numbers is 31, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (9 + 6 + 9 + 7) is 31, not 24.\n",
      "AR image scores: [None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  80%|████████  | 8/10 [00:37<00:09,  4.63s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_109.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. Top left number: 6\n",
      "2. Top right number: 8\n",
      "3. Bottom left number: 9\n",
      "4. Bottom right number: 7\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 6 + 8 + 9 + 7 = 30 \\]\n",
      "\n",
      "The sum of the four numbers is 30, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the four numbers (6 + 8 + 9 + 7) is 30, not 24.\n",
      "AR image scores: [None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images:  90%|█████████ | 9/10 [00:41<00:04,  4.55s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_158.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. Top-left number: 2\n",
      "2. Top-right number: 4\n",
      "3. Bottom-left number: 6\n",
      "4. Bottom-right number: 6\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 2 + 4 + 6 + 6 = 18 \\]\n",
      "\n",
      "The sum of the numbers is 18, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (2 + 4 + 6 + 6) is 18, not 24.\n",
      "AR image scores: [None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DDPM Images: 100%|██████████| 10/10 [00:46<00:00,  4.64s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating original image sunshadow_160.png: Score not found in output: Let's break down the image into four numbers:\n",
      "\n",
      "1. The top-left number appears to be 9.\n",
      "2. The top-right number appears to be 5.\n",
      "3. The bottom-left number appears to be 6.\n",
      "4. The bottom-right number appears to be 8.\n",
      "\n",
      "Now, let's add these numbers together:\n",
      "\\[ 9 + 5 + 6 + 8 = 28 \\]\n",
      "\n",
      "The sum of the numbers is 28, which is not equal to 24.\n",
      "\n",
      "Therefore, the label is {0}. The reason is that the sum of the numbers (9 + 5 + 6 + 8) is 28, not 24.\n",
      "AR image scores: [None, None, None, None, None, None, None, None, None, None]\n",
      "AR image scores: [None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 用于存储 Qwen 得分\n",
    "scores_original_1 = []\n",
    "running_sum = 0\n",
    "valid_count = 0\n",
    "pbar = tqdm(sampled_files_1, desc=\"Processing DDPM Images\")\n",
    "for image_name in pbar:\n",
    "    image_path = os.path.join(image_dir_1, image_name)\n",
    "    try:\n",
    "        score = evaluate_image(image_path,image_size)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating original image {image_name}: {e}\")\n",
    "        score = None\n",
    "\n",
    "    if score is not None:\n",
    "        running_sum += score\n",
    "        valid_count += 1\n",
    "\n",
    "    scores_original_1.append(score)\n",
    "    print(\"AR image scores:\", scores_original_1)\n",
    "    avg_score = running_sum / valid_count if valid_count > 0 else 0\n",
    "    pbar.set_postfix(avg_score=f\"{avg_score:.4f}\")\n",
    "print(\"AR image scores:\", scores_original_1)\n",
    "# [2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def evaluate_image(image_path,image_size) -> str:\n",
    "    \"\"\"\n",
    "    Evaluates the given image for internal visual consistency and returns a score.\n",
    "    \n",
    "    The evaluation is performed by prompting the model with a message asking to rate the image\n",
    "    from 0 (inconsistent) to 10 (completely harmonious and consistent), following the format:\n",
    "    \"The score is {your_score}. The reason is {your_reason}.\"\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): The path to the image.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated output text containing the score and the reason.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path,\"resized_height\":image_size,\"resized_width\":image_size},\n",
    "                {\"type\": \"text\", \"text\": (\n",
    "                    \"You are an expert in visual anomaly detection, focusing primarily on the shape and outline of objects. You will see images of an object in front of a mirror and its reflection. Specifically, please pay close attention to the outline and shape differences between the object in front of the mirror and its reflection, and consider whether they could reasonably be the front and back of the same object. If you think any inconsistency stems solely from color or texture design differences between the object and its reflection, please disregard it, since the front and back (or reflection) of an object may naturally vary in these aspects. Also keep in mind that perspective can cause some shape distortion in the reflection, but the overall outline should remain consistent.\"\n",
    "                    \"Please rate each image individually on a scale from 0 (inconsistent) to 10 (completely harmonious and consistent). The specific scoring criteria are as follows:\"\n",
    "                    \"Score 10: Perfect match in shape and outline, considering the mirror flip.\"  \n",
    "                    \"Score 9: Minor distortions due to perspective, but overall shape is recognizable.\"  \n",
    "                    \"Score 8: Slight inconsistencies in shape, possibly due to object positioning or minor defects.\"  \n",
    "                    \"Score 7: Moderate inconsistencies, where the general shape is similar but there are noticeable differences.\"  \n",
    "                    \"Score 6: Significant inconsistencies, making it hard to recognize the reflection as the object’s reflection.\"  \n",
    "                    \"Score 5: Equal parts consistent and inconsistent, borderline case.\"  \n",
    "                    \"Score 4: Reflection shows a completely different shape, but some elements are similar.\"  \n",
    "                    \"Score 3: Reflection bears little resemblance to the object.\"  \n",
    "                    \"Score 2: Only a few parts of the reflection match the object.\"  \n",
    "                    \"Score 1: Almost no similarity in shape and outline.\"  \n",
    "                    \"Score 0: Reflection is completely different, no similarity at all.\"\n",
    "                    \"Please think this step by step and answer in the the format: The score is {your_score}. The reason is {your_reason}.\"\n",
    "                    # \"You are an expert in visual anomaly detection, focusing primarily on the shape and outline of objects. You will see an image of an object in front of a mirror and its reflection.\" \n",
    "                    # \"Specifically, please pay close attention to the outline and shape differences between the object in front of the mirror and its reflection, and consider whether they could reasonably be the front and back of the same object. If you think any inconsistency stems solely from color or texture design differences between the object and its reflection, please disregard it, since the front and back (or reflection) of an object may naturally vary in these aspects. Also keep in mind that perspective can cause some shape distortion in the reflection, but the overall outline should remain consistent.\"\n",
    "                    # \"Please rate each image individually on a scale from 0 (inconsistent) to 10 (completely harmonious and consistent). \"\n",
    "                    # \"Please follow the format: The score is {your_score}. The reason is {your_reason}.\"\n",
    "                )},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=9000)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    # 返回生成的评分信息（例如字符串形式）\n",
    "\n",
    "    # print(\"image_path:\",image_path,\"output_text:\",output_text)\n",
    "    # return extract_score_after_final_answer(output_text) \n",
    "    pattern = r\"The score is (\\d+)\"\n",
    "    match = re.search(pattern, output_text[0])\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        raise ValueError(f\"Score not found in output: {output_text[0]}\")\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_score_after_final_answer(output_text):\n",
    "    \"\"\"\n",
    "    从给定文本中查找 **Final Answer** 后面出现的第一个数字，并返回其整数值。\n",
    "    如果未找到，则抛出 ValueError。\n",
    "    \"\"\"\n",
    "    # 若 output_text 是列表，则默认取第一个元素\n",
    "    if isinstance(output_text, list):\n",
    "        text_to_search = output_text[0]\n",
    "    else:\n",
    "        text_to_search = output_text\n",
    "\n",
    "    # 正则：匹配 \"**Final Answer**\" 后，任意字符(含换行)，再遇到数字(\\d+)\n",
    "    pattern = r\"\\*\\*Final Answer\\*\\*.*?(\\d+)\"\n",
    "    match = re.search(pattern, text_to_search, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        raise ValueError(f\"No number found after **Final Answer** in: {text_to_search}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "image_dir = \"/cpfs04/user/hanyujin/rule-gen/datasets/mirrors/left\"\n",
    "image_dir_1 = \"/cpfs04/user/hanyujin/rule-gen/experiments/samples/AE-Diff-16-mirror-SiT-B-1-linear-vae0040000.pt-size-64cfg-4.0-seed-0\"\n",
    "image_dir_2 = \"/cpfs04/user/hanyujin/rule-gen/experiments/samples/AE-JEPA-Diff-16-mirror-SiT-B-1-linear-vae0040000.pt-size-64cfg-4.0-seed-0\"\n",
    "# 获取所有 PNG 文件\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith(\".png\")]\n",
    "image_files_1 = [f for f in os.listdir(image_dir_1) if f.endswith(\".png\")]\n",
    "image_files_2 = [f for f in os.listdir(image_dir_2) if f.endswith(\".png\")]\n",
    "# 随机采样100张图片（如果图片总数不足，则全部使用）\n",
    "sample_size = min(10, len(image_files))\n",
    "sampled_files = random.sample(image_files, sample_size) #sorted(image_files)[:sample_size] #random.sample(image_files_1, sample_size)\n",
    "\n",
    "sample_size = min(10, len(image_files_1))\n",
    "sampled_files_1 = random.sample(image_files_1, sample_size) #sorted(image_files_1)[:sample_size] #random.sample(image_files_1, sample_size)\n",
    "\n",
    "sample_size = min(10, len(image_files_2))\n",
    "sampled_files_2 = random.sample(image_files_2, sample_size) #sorted(image_files_2)[:sample_size] #random.sample(image_files_2, sample_size)\n",
    "\n",
    "image_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raw Images:   0%|          | 0/10 [02:24<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, image_name)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError evaluating original image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 60\u001b[0m, in \u001b[0;36mevaluate_image\u001b[0;34m(image_path, image_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Inference: Generation of the output\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m generated_ids_trimmed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     62\u001b[0m     out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids) :] \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs\u001b[38;5;241m.\u001b[39minput_ids, generated_ids)\n\u001b[1;32m     63\u001b[0m ]\n\u001b[1;32m     64\u001b[0m output_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     65\u001b[0m     generated_ids_trimmed, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     66\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/generation/utils.py:2326\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2318\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2319\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2320\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2321\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2322\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2323\u001b[0m     )\n\u001b[1;32m   2325\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2326\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2337\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2338\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2339\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2340\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2341\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2342\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2343\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/generation/utils.py:3289\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3287\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3289\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3292\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3293\u001b[0m     outputs,\n\u001b[1;32m   3294\u001b[0m     model_kwargs,\n\u001b[1;32m   3295\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3296\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1731\u001b[0m, in \u001b[0;36mQwen2VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position)\u001b[0m\n\u001b[1;32m   1728\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39madd(delta)\n\u001b[1;32m   1729\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1745\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1152\u001b[0m, in \u001b[0;36mQwen2VLModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1141\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1142\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1149\u001b[0m         position_embeddings,\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1152\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:897\u001b[0m, in \u001b[0;36mQwen2VLDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    896\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 897\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    900\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:496\u001b[0m, in \u001b[0;36mQwen2MLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 496\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:171\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 171\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:361\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    354\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    355\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[1;32m    357\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[1;32m    358\u001b[0m         ):\n\u001b[1;32m    359\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[0;32m--> 361\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    371\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    372\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/utils/modeling.py:330\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    328\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 330\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# image_size = 64\n",
    "scores_original = []\n",
    "running_sum = 0\n",
    "valid_count = 0\n",
    "pbar = tqdm(sampled_files, desc=\"Processing Raw Images\")\n",
    "for image_name in pbar:\n",
    "    image_path = os.path.join(image_dir, image_name)\n",
    "    try:\n",
    "        score = evaluate_image(image_path,image_size)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating original image {image_name}: {e}\")\n",
    "        score = None\n",
    "\n",
    "    if score is not None:\n",
    "        running_sum += score\n",
    "        valid_count += 1\n",
    "\n",
    "    scores_original.append(score)\n",
    "    print(\"Train image scores:\", scores_original)\n",
    "    avg_score = running_sum / valid_count if valid_count > 0 else 0\n",
    "    pbar.set_postfix(avg_score=f\"{avg_score:.4f}\")\n",
    "print(\"Train image scores:\", scores_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0 7.444444444444445 8.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [10, 8, 10, 10, 10, 10,6,8]\n",
    "b = [9, 4, 6, 10, 8, 7, 7, 9, 7]\n",
    "c = [10, 9, 8, 10, 10, 2, 8, 9, 9, 7]\n",
    "print(np.mean(a),np.mean(b),np.mean(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = [10, 8, 10, 10, 10, 10,6,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AE Images:  10%|█         | 1/10 [01:39<14:57, 99.68s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path: /cpfs04/user/hanyujin/rule-gen/experiments/samples/AE-Diff-16-mirror-SiT-B-1-linear-vae0040000.pt-size-64cfg-4.0-seed-0/000001.png output_text: [\"The score is 9. The reason is that the object appears to be a long, thin rod with a small protrusion near one end. Its reflection in the mirror maintains the same elongated shape and the position of the protrusion seems consistent with what would be expected from a mirror flip. There are minor distortions likely due to perspective, but the overall shape and outline are recognizable as the reflection of the same object. The slight differences do not significantly alter the perception of the object's identity in its reflection.\"]\n",
      "Error evaluating original image 000001.png: No number found after **Final Answer** in: The score is 9. The reason is that the object appears to be a long, thin rod with a small protrusion near one end. Its reflection in the mirror maintains the same elongated shape and the position of the protrusion seems consistent with what would be expected from a mirror flip. There are minor distortions likely due to perspective, but the overall shape and outline are recognizable as the reflection of the same object. The slight differences do not significantly alter the perception of the object's identity in its reflection.\n",
      "AE image scores: [None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AE Images:  20%|██        | 2/10 [02:52<11:11, 83.95s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path: /cpfs04/user/hanyujin/rule-gen/experiments/samples/AE-Diff-16-mirror-SiT-B-1-linear-vae0040000.pt-size-64cfg-4.0-seed-0/000973.png output_text: ['The score is 7. The reason is that while the general shape of the object and its reflection are similar, there are noticeable differences in the outline and details. The reflection appears slightly distorted, which might be due to the angle and perspective of the mirror. However, these inconsistencies are moderate enough to make the reflection recognizably related to the object, but not perfectly consistent.']\n",
      "Error evaluating original image 000973.png: No number found after **Final Answer** in: The score is 7. The reason is that while the general shape of the object and its reflection are similar, there are noticeable differences in the outline and details. The reflection appears slightly distorted, which might be due to the angle and perspective of the mirror. However, these inconsistencies are moderate enough to make the reflection recognizably related to the object, but not perfectly consistent.\n",
      "AE image scores: [None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AE Images:  30%|███       | 3/10 [04:02<09:04, 77.73s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path: /cpfs04/user/hanyujin/rule-gen/experiments/samples/AE-Diff-16-mirror-SiT-B-1-linear-vae0040000.pt-size-64cfg-4.0-seed-0/000619.png output_text: ['The score is 7. The reason is that while the general shape of the motorcycle and its reflection are similar, there are noticeable differences in the details. The reflection appears slightly distorted, which could be due to the angle and perspective of the mirror. However, the overall outline is still recognizable as the same object, indicating a moderate level of consistency with some discrepancies.']\n",
      "Error evaluating original image 000619.png: No number found after **Final Answer** in: The score is 7. The reason is that while the general shape of the motorcycle and its reflection are similar, there are noticeable differences in the details. The reflection appears slightly distorted, which could be due to the angle and perspective of the mirror. However, the overall outline is still recognizable as the same object, indicating a moderate level of consistency with some discrepancies.\n",
      "AE image scores: [None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AE Images:  40%|████      | 4/10 [05:27<08:03, 80.58s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path: /cpfs04/user/hanyujin/rule-gen/experiments/samples/AE-Diff-16-mirror-SiT-B-1-linear-vae0040000.pt-size-64cfg-4.0-seed-0/000900.png output_text: [\"The score is 7. The reason is that while the general shape and outline of the object and its reflection are similar, there are noticeable differences in the details. The reflection appears slightly distorted, which might be due to the angle and perspective of the mirror. However, these differences are not so significant as to make the reflection unrecognizable as the object's reflection. The overall consistency is moderate, with some discrepancies in the finer details.\"]\n",
      "Error evaluating original image 000900.png: No number found after **Final Answer** in: The score is 7. The reason is that while the general shape and outline of the object and its reflection are similar, there are noticeable differences in the details. The reflection appears slightly distorted, which might be due to the angle and perspective of the mirror. However, these differences are not so significant as to make the reflection unrecognizable as the object's reflection. The overall consistency is moderate, with some discrepancies in the finer details.\n",
      "AE image scores: [None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AE Images:  50%|█████     | 5/10 [06:51<06:48, 81.68s/it, avg_score=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path: /cpfs04/user/hanyujin/rule-gen/experiments/samples/AE-Diff-16-mirror-SiT-B-1-linear-vae0040000.pt-size-64cfg-4.0-seed-0/000314.png output_text: ['The score is 9. The reason is that the object and its reflection show a high degree of consistency in shape and outline, with only minor distortions likely caused by perspective. The reflection accurately represents the flipped version of the object, maintaining the essential features such as the rectangular body and the protruding elements. Any slight discrepancies appear to be within the range expected due to the angle of reflection and the surface characteristics of the mirror.']\n",
      "Error evaluating original image 000314.png: No number found after **Final Answer** in: The score is 9. The reason is that the object and its reflection show a high degree of consistency in shape and outline, with only minor distortions likely caused by perspective. The reflection accurately represents the flipped version of the object, maintaining the essential features such as the rectangular body and the protruding elements. Any slight discrepancies appear to be within the range expected due to the angle of reflection and the surface characteristics of the mirror.\n",
      "AE image scores: [None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AE Images:  50%|█████     | 5/10 [08:08<08:08, 97.80s/it, avg_score=0.0000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir_1, image_name)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError evaluating original image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 60\u001b[0m, in \u001b[0;36mevaluate_image\u001b[0;34m(image_path, image_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Inference: Generation of the output\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m generated_ids_trimmed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     62\u001b[0m     out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids) :] \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs\u001b[38;5;241m.\u001b[39minput_ids, generated_ids)\n\u001b[1;32m     63\u001b[0m ]\n\u001b[1;32m     64\u001b[0m output_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     65\u001b[0m     generated_ids_trimmed, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     66\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/generation/utils.py:2326\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2318\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2319\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2320\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2321\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2322\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2323\u001b[0m     )\n\u001b[1;32m   2325\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2326\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2337\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2338\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2339\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2340\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2341\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2342\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2343\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/generation/utils.py:3289\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3287\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3289\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3292\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3293\u001b[0m     outputs,\n\u001b[1;32m   3294\u001b[0m     model_kwargs,\n\u001b[1;32m   3295\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3296\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1861\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39madd(delta)\n\u001b[1;32m   1859\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1861\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1874\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1875\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1207\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1196\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1197\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1204\u001b[0m         position_embeddings,\n\u001b[1;32m   1205\u001b[0m     )\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1207\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1218\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1084\u001b[0m, in \u001b[0;36mQwen2_5_VLDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m   1083\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m-> 1084\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m   1087\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:639\u001b[0m, in \u001b[0;36mQwen2MLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 639\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:171\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 171\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:361\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    354\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    355\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[1;32m    357\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[1;32m    358\u001b[0m         ):\n\u001b[1;32m    359\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[0;32m--> 361\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    371\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    372\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/utils/modeling.py:330\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    328\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 330\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 用于存储 Qwen 得分\n",
    "scores_original_1 = []\n",
    "running_sum = 0\n",
    "valid_count = 0\n",
    "pbar = tqdm(sampled_files_1, desc=\"Processing AE Images\")\n",
    "for image_name in pbar:\n",
    "    image_path = os.path.join(image_dir_1, image_name)\n",
    "    try:\n",
    "        score = evaluate_image(image_path,image_size)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating original image {image_name}: {e}\")\n",
    "        score = None\n",
    "\n",
    "    if score is not None:\n",
    "        running_sum += score\n",
    "        valid_count += 1\n",
    "\n",
    "    scores_original_1.append(score)\n",
    "    print(\"AE image scores:\", scores_original_1)\n",
    "    avg_score = running_sum / valid_count if valid_count > 0 else 0\n",
    "    pbar.set_postfix(avg_score=f\"{avg_score:.4f}\")\n",
    "print(\"AE image scores:\", scores_original_1)\n",
    "# [2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AE-JEPA Images:   0%|          | 0/10 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir_2, image_name)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError evaluating original image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 60\u001b[0m, in \u001b[0;36mevaluate_image\u001b[0;34m(image_path, image_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Inference: Generation of the output\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m generated_ids_trimmed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     62\u001b[0m     out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids) :] \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs\u001b[38;5;241m.\u001b[39minput_ids, generated_ids)\n\u001b[1;32m     63\u001b[0m ]\n\u001b[1;32m     64\u001b[0m output_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     65\u001b[0m     generated_ids_trimmed, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     66\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/generation/utils.py:2326\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2318\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2319\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2320\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2321\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2322\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2323\u001b[0m     )\n\u001b[1;32m   2325\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2326\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2337\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2338\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2339\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2340\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2341\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2342\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2343\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/generation/utils.py:3289\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3287\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3289\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3292\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3293\u001b[0m     outputs,\n\u001b[1;32m   3294\u001b[0m     model_kwargs,\n\u001b[1;32m   3295\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3296\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1861\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39madd(delta)\n\u001b[1;32m   1859\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1861\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1874\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1875\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1207\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1196\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1197\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1204\u001b[0m         position_embeddings,\n\u001b[1;32m   1205\u001b[0m     )\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1207\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1218\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1084\u001b[0m, in \u001b[0;36mQwen2_5_VLDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m   1083\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m-> 1084\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m   1087\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:639\u001b[0m, in \u001b[0;36mQwen2MLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 639\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:171\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 171\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/hooks.py:361\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    354\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    355\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[1;32m    357\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[1;32m    358\u001b[0m         ):\n\u001b[1;32m    359\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[0;32m--> 361\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    371\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    372\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/yjenv/lib/python3.11/site-packages/accelerate/utils/modeling.py:330\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    328\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 330\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores_original_2 = []\n",
    "running_sum = 0\n",
    "valid_count = 0\n",
    "pbar = tqdm(sampled_files_2, desc=\"Processing AE-JEPA Images\")\n",
    "for image_name in pbar:\n",
    "    image_path = os.path.join(image_dir_2, image_name)\n",
    "    try:\n",
    "        score = evaluate_image(image_path,image_size)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating original image {image_name}: {e}\")\n",
    "        score = None\n",
    "\n",
    "    if score is not None:\n",
    "        running_sum += score\n",
    "        valid_count += 1\n",
    "\n",
    "    scores_original_2.append(score)\n",
    "    print(\"JEPA image scores:\", scores_original_2)\n",
    "    avg_score = running_sum / valid_count if valid_count > 0 else 0\n",
    "    pbar.set_postfix(avg_score=f\"{avg_score:.4f}\")\n",
    "\n",
    "print(\"JEPA image scores:\", scores_original_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path: /cpfs04/user/hanyujin/rule-gen/datasets/mirrors/left/12526.png output_text: [\"Alright, I've got this task where I need to look at images of an object in front of a mirror and its reflection, and rate how consistent the shape and outline are between the object and its reflection. The rating scale is from 0 to 10, with 0 meaning completely inconsistent and 10 meaning perfectly harmonious and consistent.\\n\\nFirst, I need to understand what exactly is being asked. I have to focus on the shape and outline of the object and its reflection. If there are differences in color or texture between the object and its reflection, I should ignore those because those can naturally vary between the front and back of an object. So, my main concern is the geometric consistency.\\n\\nLet me think about how mirrors work. When an object is placed in front of a mirror, the reflection should mirror the object's shape across the plane of the mirror. So, if the object is symmetrical, its reflection should look similar or identical, depending on the type of symmetry.\\n\\nBut in this case, the object might not be symmetrical, and that's where potential inconsistencies could arise. I need to carefully observe the shape of the object and compare it with its reflection to see if they align properly when considering the mirror's position.\\n\\nPerspective can also play a role here. Depending on the angle and distance of the object from the mirror, the reflection might appear distorted. I need to account for that and try to visualize the object's true reflection based on its actual shape.\\n\\nMaybe it would help to imagine folding the object along the mirror line to see if the reflection matches the other side. If they don't match up perfectly in terms of shape and outline, that could indicate an inconsistency.\\n\\nAlso, I should consider the lighting conditions. If the lighting is uneven, it might affect how the object and its reflection appear, potentially causing shadows or highlights that could be misleading.\\n\\nWait, but the instruction says to disregard color and texture differences, so I should focus solely on the shape and outline, not on surface properties.\\n\\nLet me consider some examples to better understand.\\n\\nSuppose there's an object that is perfectly symmetrical, like a sphere. Its reflection in the mirror should look identical to itself, regardless of the angle or position. So, in that case, the consistency would be high, probably a 10.\\n\\nNow, if the object is asymmetrical, like a hand, the reflection would show the mirror image, which might not look like the front side. In that case, I need to imagine flipping the object over the mirror line to see if it matches the reflection.\\n\\nIf the reflection doesn't match the expected mirror image due to some distortion or difference in shape, then I should rate it lower.\\n\\nAnother scenario could be an object with text on it. If the text is readable in the object but appears backward in the reflection, that's consistent because that's how mirrors work with text.\\n\\nBut if the reflection shows text that doesn't match the expected mirror image, that would be inconsistent.\\n\\nWait, but the instruction mentions that the object could have design differences between the front and back, so if the front has text and the back has a different design, I should ignore that because it's natural for front and back to differ in design.\\n\\nSo, in that case, even if the reflection shows a different design, as long as it matches the actual back of the object, it should be considered consistent.\\n\\nHmm, that adds another layer of complexity. I need to make sure that I'm only judging the shape and outline, and not the surface appearance or design elements.\\n\\nPerhaps it would help to think of the object as having a 3D shape, and the reflection should mirror that shape across the mirror plane.\\n\\nI should also consider the depth and proportions. If the reflection appears stretched or compressed compared to the actual object, that could indicate an inconsistency in shape.\\n\\nLet me think about how to quantify this consistency. Since it's a scale from 0 to 10, I need to define what constitutes each level of consistency.\\n\\n- 0: No similarity in shape between the object and its reflection.\\n\\n- 5: Moderate similarity, with some noticeable inconsistencies in shape or outline.\\n\\n- 10: Perfect match in shape and outline between the object and its reflection.\\n\\nSo, for each image, I need to assess where it falls on this scale based on the shape and outline consistency.\\n\\nMaybe I can also consider partial consistency. For example, if parts of the object reflect correctly but other parts don't, I can assign a score based on the overall percentage of consistency.\\n\\nFor instance, if 70% of the object's reflection matches its shape and outline, I could give it a 7.\\n\\nBut I need to be careful not to overcomplicate things. The task seems straightforward: focus on shape and outline, ignore color and texture differences, and rate the consistency on a 0-10 scale.\\n\\nI should also remember that perspective can affect the appearance of the reflection. If the object is not perpendicular to the mirror, the reflection might appear distorted.\\n\\nIn such cases, I need to adjust for the perspective and consider what the reflection should look like given the object's orientation.\\n\\nPerhaps referencing geometric principles could help. For example, understanding how shapes behave when reflected across a plane.\\n\\nLet me consider a simple example: a rectangular box placed in front of a mirror.\\n\\nIf the box is symmetrical, its reflection should look similar to itself, perhaps with some differences in perspective.\\n\\nIf it's asymmetrical, like different dimensions on different sides, the reflection might not match perfectly, and I need to judge the consistency based on that.\\n\\nAnother example: a sculpture with complex curves and angles. Its reflection could be more difficult to predict, but I still need to assess if the reflection matches the object's shape and outline reasonably well.\\n\\nI should also consider the clarity of the reflection. If the reflection is blurry or distorted due to the mirror's quality or lighting conditions, that might affect my judgment.\\n\\nHowever, assuming that the images are clear and well-lit, I can focus solely on the shape and outline without being swayed by image quality issues.\\n\\nLet me think about potential pitfalls in this task.\\n\\nOne pitfall could be allowing personal bias to influence the rating. For example, if I find a particular shape aesthetically pleasing, I might rate it higher, even if the reflection isn't perfectly consistent.\\n\\nTo avoid that, I need to stay objective and base my ratings solely on the consistency of shape and outline between the object and its reflection.\\n\\nAnother pitfall could be overemphasizing small discrepancies. If there are minor inconsistencies due to manufacturing imperfections or wear and tear, I shouldn't let that skew my rating too much.\\n\\nInstead, I should consider the overall consistency and give a fair score based on that.\\n\\nAlso, I need to remember that mirrors can sometimes create optical illusions, making shapes appear different than they actually are.\\n\\nI need to be aware of these illusions and try to see beyond them to assess the true shape consistency.\\n\\nLet me try to summarize my approach:\\n\\n1. Focus on the shape and outline of the object and its reflection.\\n\\n2. Ignore color and texture differences, as they can vary between the front and back of an object.\\n\\n3. Account for perspective distortion based on the object's position relative to the mirror.\\n\\n4. Rate the consistency on a 0-10 scale, with 0 being completely inconsistent and 10 being perfectly consistent.\\n\\n5. Stay objective and base the rating on the overall consistency, allowing for minor discrepancies.\\n\\nBy following these steps, I should be able to provide accurate and fair ratings for each image.\\n\\nNow, I'm ready to proceed with evaluating the images provided.\\n\\n**Final Answer**\\n\\n\\\\[ \\\\boxed{\\\\text{The score is } 10. \\\\text{ The reason is the shape and outline perfectly match between the object and its reflection.}} \\\\]\"]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "image_size = 64\n",
    "raw_path = \"/cpfs04/user/hanyujin/rule-gen/datasets/mirrors/left/12526.png\" #03760.png\n",
    "score =  evaluate_image(raw_path,image_size)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path: /cpfs04/user/hanyujin/rule-gen/experiments/samples/AE-Diff-16-mirror-SiT-B-1-linear-vae0040000.pt-size-64cfg-4.0-seed-0/000000.png output_text: [\"The score is 3. The reason is that the image is quite blurry, making it difficult to discern fine details and compare the outlines and shapes accurately. However, there appears to be a noticeable difference in the structure and alignment of the object and its reflection, suggesting inconsistency. The object seems to have a more defined shape compared to its reflection, which looks distorted and less clear. This discrepancy suggests that the two might not be the front and back of the same object or that the reflection is not accurately representing the object's features.\"]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "ae_path = \"/cpfs04/user/hanyujin/rule-gen/experiments/samples/AE-Diff-16-mirror-SiT-B-1-linear-vae0040000.pt-size-64cfg-4.0-seed-0/000000.png\"\n",
    "score =  evaluate_image(ae_path,image_size)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path: /cpfs04/user/hanyujin/rule-gen/experiments/samples/AE-JEPA-Diff-16-mirror-SiT-B-1-linear-vae0040000.pt-size-64cfg-4.0-seed-0/000000.png output_text: ['The score is 3. The reason is that the image is quite blurry, making it difficult to discern detailed outlines and shapes. However, based on the visible elements, there appears to be a noticeable difference in the shapes and positions of the objects and their reflections. The object in front of the mirror seems to have a more defined and upright shape compared to its reflection, which looks distorted and inconsistent with what would be expected as a reflection of the same object. This inconsistency suggests that the two might not represent the front and back of the same object accurately.']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "jepa_ae_path = \"/cpfs04/user/hanyujin/rule-gen/experiments/samples/AE-JEPA-Diff-16-mirror-SiT-B-1-linear-vae0040000.pt-size-64cfg-4.0-seed-0/000000.png\"\n",
    "score =  evaluate_image(jepa_ae_path,image_size)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 20/20 [04:11<00:00, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image scores: [8, 9, 8, 8, 9, 8, 8, 8, 5, 8, 9, 8, 8, 8, 8, 8, 8, 8, 8, 7]\n",
      "Reflection removed image scores: [7, 8, 8, 8, 8, 8, 8, 7, 8, 8, 9, 8, 8, 8, 8, 8, 8, 8, 8, 7]\n",
      "Rotated image scores: [8, 8, 9, 8, 9, 8, 8, 7, 8, 8, 9, 8, 2, 8, 8, 7, 8, 8, 8, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "\n",
    "# 假设 evaluate_image 已经定义，输入为图片路径，返回 Qwen score（例如 int 型分数）\n",
    "# def evaluate_image(image_path: str) -> int:\n",
    "#     ...\n",
    "\n",
    "# 图像文件夹路径\n",
    "image_dir = \"/cpfs04/user/hanyujin/rule-gen/datasets/mirrors/left\"\n",
    "\n",
    "# 获取所有 PNG 文件\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith(\".png\")]\n",
    "\n",
    "# 随机采样100张图片（如果图片总数不足，则全部使用）\n",
    "sample_size = min(20, len(image_files))\n",
    "sampled_files = random.sample(image_files, sample_size)\n",
    "\n",
    "# 用于存储 Qwen 得分\n",
    "scores_original = []\n",
    "scores_remove_reflection = []\n",
    "scores_rotation = []\n",
    "\n",
    "for image_name in tqdm(sampled_files, desc=\"Processing Images\"):\n",
    "    image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "    # 读取原始图像（image1）\n",
    "    image1 = Image.open(image_path).convert(\"RGB\")\n",
    "    image1_np = np.array(image1)\n",
    "\n",
    "    # 生成去反射图（image2）\n",
    "    h, w, c = image1_np.shape\n",
    "    upper_half = image1_np[:int(h//3)].copy()  # 取上半部分（这里以 h//3 作为示例区域）\n",
    "    x1, x2 = w // 3, 2 * w // 3  # 插值范围\n",
    "    for x in range(x1, x2):\n",
    "        alpha = (x - x1) / (x2 - x1)\n",
    "        # 线性插值：左右两侧的像素均值\n",
    "        upper_half[:, x] = (1 - alpha) * upper_half[:, x1 - 1] + alpha * upper_half[:, x2]\n",
    "    image2_np = image1_np.copy()\n",
    "    image2_np[:int(h//3)] = upper_half\n",
    "    image2 = Image.fromarray(image2_np.astype(np.uint8))\n",
    "\n",
    "    # 生成旋转图（image3）\n",
    "    image3 = image1.rotate(180)\n",
    "\n",
    "    # 对于 image1，直接使用原始路径进行评分\n",
    "    try:\n",
    "        score1 = evaluate_image(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating original image {image_name}: {e}\")\n",
    "        score1 = None\n",
    "\n",
    "    # 对于 image2 和 image3，由于 evaluate_image 需要图片路径，因此暂存到临时文件中\n",
    "    score2, score3 = None, None\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmp_file:\n",
    "            tmp_path2 = tmp_file.name\n",
    "            image2.save(tmp_path2)\n",
    "        score2 = evaluate_image(tmp_path2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating reflection-removed image {image_name}: {e}\")\n",
    "    finally:\n",
    "        if os.path.exists(tmp_path2):\n",
    "            os.remove(tmp_path2)\n",
    "    \n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmp_file:\n",
    "            tmp_path3 = tmp_file.name\n",
    "            image3.save(tmp_path3)\n",
    "        score3 = evaluate_image(tmp_path3)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating rotated image {image_name}: {e}\")\n",
    "    finally:\n",
    "        if os.path.exists(tmp_path3):\n",
    "            os.remove(tmp_path3)\n",
    "    \n",
    "    scores_original.append(score1)\n",
    "    scores_remove_reflection.append(score2)\n",
    "    scores_rotation.append(score3)\n",
    "\n",
    "# 打印或进一步处理得到的得分\n",
    "print(\"Original image scores:\", scores_original)\n",
    "print(\"Reflection removed image scores:\", scores_remove_reflection)\n",
    "print(\"Rotated image scores:\", scores_rotation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_original = [7, 3, 8, 7, 3, 3, 8, 2, 8, 9, 2, 8, 7, 3, 3, 3, 8, 7, 8, 8, 3, 8, 8, 8, 8, 3, 3, 3, 2, 7, 8, 8, 3, 8, 3, 9, 2, 8, 7, 3, 8, 2, 8, 2, 8, 3, 9, 2, 8, 8, 8, 3, 5, 8, 2, 8, 3, 8, 2, 8, 7, 8, 9, 2, 8, 8, 8, 7, 8, 8, 2, 7, 8, 8, 8, 7, 8, 2, 7, 7, 7, 8, 2, 5, 8, 3, 3, 3, 8, 8, 8, 8, 8, 7, 8, 8, 8, 7, 8, 7, 8, 8, 2, 9, 5, 9, 8, 7, 3, 8, 3, 2, 8, 8, 8, 5, 8, 8, 8, 8, 3, 3, 2, 7, 2, 8, 2, 3, 8, 3, 2, 7, 3, 8, 8, 4, 2, 8, 8, 3, 3, 8, 8, 8, 8, 8, 9, 8, 8, 2, 8, 2, 7, 8, 8, 9, 8, 8, 8, 8, 8, 8, 8, 2, 3, 2, 5, 3, 3, 8, 8, 8, 9, 2, 8, 3, 3, 2, 7, 8, 3, 3, 8, 3, 8, 7, 2, 7, 2, 2, 2, 8, 8, 8, 3, 8, 2, 5, 7, 7, 5, 7, 8, 8, 8, 8, 7, 8, 8, 2, 2, 7, 3, 3, 8, 3, 8, 8, 8, 7, 7, 8, 8, 7, 8, 8, 8, 8, 8, 7, 8, 3, 2, 3, 3, 8, 8, 3, 8, 8, 8, 3, 8, 3, 3, 8, 8, 2, 8, 3, 3, 8, 5, 3, 8, 8, 7, 8, 7, 8, 9, 8, 2, 2, 8, 8, 2, 3, 3, 3, 2, 9, 8, 2, 9, 7, 8, 2, 3, 3, 2, 8, 7, 8, 8, 7, 8, 7, 9, 3, 7, 3, 3, 7, 8, 8, 8, 8, 2, 8, 8, 9, 8, 8, 2, 2, 8, 8, 8, 8, 3, 8, 9, 2, 7, 8, 8, 8, 8, 9, 9, 8, 8, 3, 8, 7, 2, 8, 8, 8, 8, 3, 9, 9, 3, 9, 2, 8, 3, 8, 8, 2, 8, 8, 2, 8, 2, 7, 8, 8, 2, 8, 8, 8, 8, 5, 3, 7, 8, 7, 9, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 7, 8, 3, 8, 8, 8, 2, 3, 7, 8, 3, 9, 7, 2, 8, 8, 3, 7, 3, 8, 7, 8, 9, 2, 8, 9, 7, 7, 2, 8, 2, 2, 8, 3, 7, 8, 9, 8, 8, 3, 8, 3, 3, 8, 7, 3, 3, 2, 8, 3, 8, 8, 3, 8, 2, 4, 7, 5, 2, 3, 7, 7, 8, 8, 3, 8, 8, 8, 8, 8, 8, 3, 7, 3, 5, 8, 5, 8, 9, 7, 3, 8, 2, 5, 3, 3, 8, 8, 8, 3, 7, 8, 7, 3, 8, 2, 2, 8, 3, 8, 7, 7, 5, 8, 4, 8, 8, 2, 8, 9, 8, 7, 7, 5, 8, 8, 8, 7, 8, 8, 2, 8, 8, 7, 8, 7, 9, 3, 3, 7, 8, 8, 8, 8, 2, 7, 8, 8, 8, 3, 8, 8, 7, 2, 8, 8, 3, 8, 7, 7, 3, 8, 8, 3, 8, 8, 8, 3, 8, 3, 7, 7, 7, 7, 8, 8, 8, 7, 2, 3, 8, 8, 8, 7, 3, 8, 8, 2, 8, 8, 8, 8, 7, 5, 3, 8, 8, 8, 3, 8, 8, 8, 8, 9, 3, 2, 8, 8, 8, 3, 8, 8, 8, 3, 8, 8, 7, 8, 3, 8, 8, 8, 8, 9, 8, 8, 2, 3, 2, 3, 8, 3, 2, 8, 8, 8, 3, 3, 3, 7, 8, 8, 7, 8, 9, 8, 8, 3, 2, 8, 8, 8, 3, 3, 8, 7, 8, 8, 8, 8, 3, 8, 8, 3, 7, 2, 3, 7, 8, 8, 2, 2, 2, 7, 8, 8, 2, 3, 5, 8, 8, 8, 3, 7, 8, 2, 2, 2, 2, 8, 7, 8, 8, 5, 8, 3, 8, 8, 2, 8, 8, 8, 8, 3, 3, 2, 8, 7, 8, 7, 7, 2, 2, 5, 2, 7, 8, 8, 3, 8, 2, 8, 2, 8, 8, 8, 2, 8, 7, 3, 2, 9, 8, 7, 3, 5, 8, 7, 8, 8, 8, 8, 8, 9, 8, 7, 8, 8, 8, 7, 7, 8, 3, 9, 8, 4, 9, 3, 8, 3, 8, 3, 8, 2, 8, 3, 7, 2, 3, 3, 8, 9, 3, 8, 8, 3, 8, 7, 8, 8, 8, 2, 3, 8, 2, 8, 7, 3, 3, 7, 8, 8, 7, 8, 7, 8, 8, 8, 8, 8, 7, 8, 8, 8, 8, 8, 8, 8, 2, 8, 3, 8, 3, 8, 7, 2, 7, 2, 8, 7, 8, 8, 8, 7, 2, 8, 8, 8, 8, 7, 2, 3, 8, 3, 2, 3, 7, 3, 3, 3, 8, 8, 8, 8, 2, 7, 5, 8, 2, 3, 2, 2, 7, 8, 3, 3, 8, 8, 8, 7, 3, 8, 3, 8, 8, 8, 8, 8, 8, 8, 3, 2, 8, 8, 9, 8, 9, 8, 8, 2, 7, 8, 2, 8, 3, 2, 3, 2, 9, 8, 3, 8, 3, 8, 8, 8, 7, 8, 2, 8, 8, 8, 2, 8, 2, 7, 8, 8, 8, 8, 3, 8, 3, 8, 8, 8, 2, 3, 5, 8, 2, 8, 8, 2, 8, 3, 7, 8, 8, 9, 8, 8, 8, 9, 3, 3, 8, 8, 8, 8, 8, 7, 8, 3, 7, 2, 8, 8, 8, 8, 8, 5, 8, 8, 7, 3, 3, 8, 8, 8, 8, 9, 3, 3, 8, 8, 8, 3, 8, 3, 8, 8, 7, 2, 2, 3, 9, 8, 8, 3, 2, 2, 8, 3, 8, 8, 7, 8, 8, 3, 3, 8, 8, 8, 7, 7, 2, 8, 8, 8, 8, 8, 3, 8, 7, 2, 3, 8, 8, 8, 8, 7, 8, 2, 7, 8, 7, 2, 2, 3, 8, 8, 7, 2, 3, 9, 3, 8, 8, 8, 8, 9, 4, 7, 3, 5, 3, 8]\n",
    "scores_remove_reflection=  [7, 3, 3, 3, 2, 7, 3, 8, 8, 9, 2, 2, 3, 2, 2, 2, 8, 3, 8, 8, 3, 8, 7, 3, 5, 3, 2, 3, 2, 3, 8, 8, 3, 8, 7, 9, 3, 7, 3, 8, 8, 3, 8, 2, 3, 2, 8, 3, 8, 7, 8, 2, 8, 8, 2, 8, 3, 8, 3, 8, 7, 8, 8, 2, 7, 8, 7, 4, 9, 8, 2, 8, 7, 7, 8, 3, 8, 2, 7, 3, 3, 8, 2, 2, 8, 3, 3, 3, 8, 8, 8, 9, 7, 7, 8, 8, 3, 3, 8, 7, 8, 7, 3, 8, 8, 8, 8, 8, 7, 7, 2, 3, 8, 8, 8, 3, 8, 3, 3, 7, 7, 2, 8, 3, 2, 8, 2, 2, 8, 3, 2, 8, 3, 8, 7, 7, 2, 2, 8, 3, 3, 8, 8, 5, 8, 8, 3, 3, 8, 2, 8, 8, 8, 8, 8, 8, 8, 2, 3, 3, 3, 8, 8, 2, 3, 8, 4, 5, 7, 8, 3, 8, 9, 3, 8, 2, 8, 3, 3, 8, 3, 2, 8, 2, 7, 8, 2, 3, 3, 2, 3, 8, 3, 8, 3, 8, 3, 3, 3, 7, 2, 7, 3, 8, 3, 9, 7, 8, 8, 2, 2, 4, 2, 7, 2, 2, 8, 2, 3, 7, 3, 5, 3, 8, 8, 8, 8, 8, 3, 2, 3, 3, 2, 3, 3, 8, 7, 3, 3, 8, 8, 2, 7, 3, 2, 8, 8, 2, 8, 7, 3, 7, 5, 2, 8, 3, 3, 3, 7, 3, 9, 8, 7, 2, 8, 8, 2, 3, 2, 7, 2, 8, 8, 2, 8, 3, 7, 8, 3, 7, 8, 7, 3, 8, 8, 8, 7, 7, 8, 3, 3, 3, 2, 2, 8, 8, 8, 9, 2, 8, 8, 8, 7, 7, 2, 7, 7, 7, 8, 8, 3, 8, 9, 3, 7, 8, 8, 8, 8, 8, 8, 7, 8, 8, 8, 3, 3, 7, 5, 8, 8, 3, 8, 9, 3, 8, 2, 7, 3, 2, 8, 2, 3, 7, 8, 7, 3, 8, 7, 8, 8, 8, 8, 7, 3, 2, 2, 3, 8, 7, 8, 3, 8, 8, 8, 8, 3, 3, 8, 8, 8, 8, 3, 8, 5, 8, 2, 8, 2, 8, 7, 8, 3, 9, 7, 8, 3, 8, 3, 3, 8, 8, 7, 8, 8, 2, 8, 8, 8, 8, 3, 8, 3, 3, 7, 2, 5, 8, 9, 8, 8, 3, 8, 2, 2, 8, 3, 3, 3, 2, 3, 3, 8, 8, 2, 8, 3, 7, 8, 8, 2, 3, 3, 8, 7, 8, 3, 7, 7, 7, 2, 8, 9, 8, 8, 3, 3, 8, 2, 7, 8, 8, 3, 8, 2, 2, 3, 2, 3, 3, 8, 3, 7, 8, 7, 3, 8, 2, 8, 8, 2, 8, 8, 7, 2, 3, 3, 2, 7, 2, 9, 9, 8, 3, 3, 7, 8, 8, 8, 7, 8, 7, 2, 3, 8, 7, 8, 7, 9, 3, 3, 8, 3, 8, 8, 8, 2, 7, 8, 8, 7, 3, 7, 2, 3, 2, 8, 3, 2, 8, 3, 7, 2, 8, 8, 7, 8, 5, 8, 2, 8, 2, 7, 8, 2, 8, 8, 3, 8, 3, 2, 8, 3, 8, 8, 7, 2, 8, 8, 3, 2, 7, 7, 8, 7, 7, 2, 8, 8, 2, 3, 8, 8, 3, 8, 8, 3, 2, 8, 8, 8, 3, 7, 8, 7, 2, 8, 8, 5, 8, 3, 3, 7, 8, 8, 8, 8, 8, 3, 3, 2, 3, 8, 3, 2, 8, 3, 7, 2, 3, 2, 8, 2, 8, 2, 8, 9, 8, 8, 5, 2, 8, 8, 8, 3, 3, 8, 3, 3, 3, 8, 9, 3, 3, 8, 7, 5, 2, 5, 3, 3, 7, 2, 3, 2, 7, 8, 8, 3, 3, 5, 9, 9, 8, 2, 8, 7, 3, 2, 8, 3, 7, 7, 8, 7, 2, 8, 5, 2, 7, 8, 8, 8, 8, 8, 7, 2, 2, 3, 7, 8, 8, 7, 2, 3, 7, 2, 3, 8, 3, 3, 3, 7, 7, 3, 3, 8, 8, 3, 7, 7, 7, 3, 8, 8, 7, 2, 3, 8, 7, 8, 8, 8, 3, 8, 8, 8, 3, 3, 8, 8, 3, 7, 8, 8, 8, 7, 3, 9, 3, 8, 7, 8, 3, 8, 2, 8, 8, 7, 2, 2, 3, 8, 9, 2, 8, 8, 3, 3, 3, 3, 8, 8, 2, 7, 8, 2, 8, 2, 2, 3, 7, 8, 8, 3, 8, 2, 8, 2, 8, 8, 8, 7, 9, 8, 8, 8, 7, 7, 3, 2, 7, 2, 8, 3, 8, 3, 2, 2, 7, 7, 8, 3, 8, 8, 3, 2, 7, 8, 3, 7, 3, 2, 3, 8, 3, 3, 2, 4, 5, 8, 2, 2, 8, 7, 8, 3, 3, 8, 8, 3, 2, 2, 2, 7, 8, 3, 3, 3, 8, 7, 7, 3, 8, 3, 8, 5, 8, 8, 8, 8, 8, 3, 2, 9, 8, 8, 8, 9, 8, 9, 8, 3, 8, 3, 9, 3, 3, 8, 2, 8, 7, 8, 3, 3, 8, 3, 3, 5, 8, 2, 8, 8, 8, 2, 8, 2, 3, 8, 7, 8, 8, 8, 8, 3, 7, 8, 3, 2, 7, 3, 8, 2, 8, 8, 2, 8, 3, 7, 3, 3, 9, 8, 8, 8, 8, 3, 2, 8, 7, 2, 8, 7, 7, 8, 2, 7, 3, 8, 8, 8, 2, 8, 7, 2, 8, 8, 2, 3, 9, 9, 8, 3, 9, 2, 2, 3, 8, 8, 4, 8, 3, 8, 8, 7, 2, 2, 2, 9, 8, 3, 3, 3, 3, 8, 3, 8, 9, 8, 7, 8, 3, 2, 8, 9, 8, 7, 3, 2, 9, 8, 7, 8, 7, 2, 8, 3, 2, 3, 7, 8, 2, 7, 7, 8, 2, 3, 8, 3, 2, 2, 2, 2, 8, 7, 2, 3, 9, 8, 8, 3, 8, 8, 8, 7, 3, 2, 3, 8, 8]\n",
    "scores_rotation= [8, 5, 5, 8, 5, 8, 8, 8, 8, 2, 5, 8, 8, 8, 8, 8, 2, 8, 5, 8, 5, 8, 8, 2, 8, 2, 8, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 2, 5, 8, 7, 2, 2, 8, 5, 8, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores_remove_reflection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "data = {\n",
    "    \"Qwen2.5-VL-72B\": [\"Raw\"] * len(scores_original) + [\"Pertub\"] * len(scores_remove_reflection),\n",
    "    \"MLLM Score\": scores_original + scores_remove_reflection\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 计算 t 检验的 p-value\n",
    "t_stat, p_val = ttest_ind(scores_original, scores_remove_reflection, equal_var=False)\n",
    "p_value_text = f\"p = {p_val:.3f}\"\n",
    "\n",
    "# 设置绘图参数\n",
    "plt.rcParams[\"axes.labelsize\"] = 15\n",
    "palette = ['#0073C2FF', '#EFC000FF']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=100, facecolor=\"w\")\n",
    "ax = sns.barplot(\n",
    "    x=\"Qwen2.5-VL-72B\", y=\"MLLM Score\", data=df, palette=palette,\n",
    "    estimator=np.mean, ci=\"sd\", capsize=.1, errwidth=1, errcolor=\"k\",\n",
    "    ax=ax, edgecolor=\"k\", linewidth=1\n",
    ")\n",
    "\n",
    "# 添加 p-value 标注\n",
    "x1, x2 = 0, 1  # 两个类别在 x 轴上的位置\n",
    "# 确定标注线的高度：取两个组均值加标准差中的最大值，再加上一些偏移\n",
    "y = max(np.mean(scores_original) + np.std(scores_original), np.mean(scores_remove_reflection) + np.std(scores_remove_reflection)) + 1\n",
    "h = 0.2  # 横线的高度偏移\n",
    "ax.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1, c=\"k\")\n",
    "ax.text((x1+x2)*.5, y+h, \"T-test: \" + p_value_text, ha='center', va='bottom', color=\"k\")\n",
    "\n",
    "# 调整坐标轴和网格\n",
    "ax.tick_params(which='major', direction='in', length=3, width=1., labelsize=14, bottom=False)\n",
    "for spine in [\"top\", \"left\", \"right\"]:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "ax.spines['bottom'].set_linewidth(2)\n",
    "ax.grid(axis='y', ls='--', c='gray')\n",
    "ax.set_axisbelow(True)\n",
    "plt.savefig(\"/cpfs04/user/hanyujin/rule-gen/rule_tokenizer/eval/mllmscore/mmlmscore_Qwen2.5-VL-72B-Instruct.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.58"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(scores_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 假设这三个列表已经定义\n",
    "# scores_original, scores_remove_reflection, scores_rotation\n",
    "\n",
    "# 计算每个列表的均值和标准差\n",
    "data = [scores_original, scores_remove_reflection, scores_rotation]\n",
    "labels = [\"Raw\", \"Remove\", \"Rotate\"]\n",
    "\n",
    "means = [np.mean(d) for d in data]\n",
    "stds = [np.std(d) for d in data]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, means, yerr=stds, capsize=10, color=[\"blue\", \"orange\", \"green\"])\n",
    "plt.ylabel(\"CLIP Score\")\n",
    "plt.title(\"CLIP Score Comparison (1000 Samples)\")\n",
    "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "# plt.savefig(\"/cpfs04/user/hanyujin/rule-gen/rule_tokenizer/eval/mllmscore/mmlmscore.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/yjenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-25 10:52:52.221996: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-25 10:52:52.244759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742871172.263006 3517069 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742871172.268269 3517069 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742871172.286803 3517069 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742871172.286826 3517069 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742871172.286828 3517069 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742871172.286829 3517069 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-25 10:52:52.291308: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards: 100%|██████████| 38/38 [00:43<00:00,  1.13s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Processing Batches:  75%|███████▌  | 3/4 [14:02<04:38, 278.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch: No number found after **Final Answer** in: Alright, I've got this task where I need to look at images of objects in front of a mirror and evaluate if their shape and outline match their reflection. I should ignore colors and textures and just focus on the shape and outline. I have to rate each image on a scale from 0 to 10.\n",
      "\n",
      "First, I need to understand what exactly is being asked. The object is placed in front of a mirror, and there's a reflection. I need to check if the reflection accurately represents the shape and outline of the object. So, basically, I need to see if the mirror is showing the correct form of the object.\n",
      "\n",
      "I should consider things like symmetry, angles, and proportions. If the object is symmetrical, its reflection should match perfectly. If it's asymmetrical, I need to make sure that the reflection corresponds correctly to the object's shape.\n",
      "\n",
      "Let me think about how mirrors work. A mirror reflects light, and the image it produces is a virtual image that appears to be behind the mirror. The distance of the image from the mirror is equal to the distance of the object from the mirror. Also, the size of the image is the same as the object's size. So, no scaling should be happening unless the mirror is curved, which I assume it's not in these images.\n",
      "\n",
      "I should also consider if there are any distortions caused by the mirror's surface or if the mirror is perfectly flat. Any imperfections in the mirror could affect the reflection.\n",
      "\n",
      "Another thing to consider is the perspective. If the object is not perpendicular to the mirror, the reflection might appear distorted. For example, if the object is tilted, its reflection will also be tilted accordingly.\n",
      "\n",
      "I need to pay attention to the edges of the object and ensure that they align properly with the edges in the reflection. Any discrepancies there could indicate a problem with the reflection's accuracy.\n",
      "\n",
      "Also, I should check for any parts of the object that might be obscured or cut off in the reflection. The entire object should be reflected fully unless it's intentionally being cropped by the mirror's edge.\n",
      "\n",
      "Since I'm supposed to ignore color and texture, I can focus solely on the outline and shape. So, even if the colors don't match or the textures are different, as long as the shape and outline are correct, I should give it a higher score.\n",
      "\n",
      "Let me try to establish some criteria for scoring:\n",
      "\n",
      "- 10: Perfect match in shape and outline between the object and its reflection.\n",
      "\n",
      "- 9: Minor inconsistencies in outline but overall shape is preserved.\n",
      "\n",
      "- 8: Some noticeable discrepancies in the outline, but the reflection is still recognizable.\n",
      "\n",
      "- 7: Significant differences in the outline, making the reflection less accurate.\n",
      "\n",
      "- 6: Reflection shows substantial deviations from the object's shape.\n",
      "\n",
      "- 5: Reflection is almost as expected but with notable errors in shape and outline.\n",
      "\n",
      "- 4: Reflection bears some resemblance but is largely inaccurate.\n",
      "\n",
      "- 3: Only parts of the reflection match the object's shape.\n",
      "\n",
      "- 2: Reflection is barely recognizable; shape and outline are poorly represented.\n",
      "\n",
      "- 1: Reflection is completely different from the object's shape and outline.\n",
      "\n",
      "- 0: No reflection or completely missing object.\n",
      "\n",
      "I need to make sure that I'm consistent in applying these criteria across all images.\n",
      "\n",
      "Now, I should look at each image one by one and apply this evaluation.\n",
      "\n",
      "Starting with the first image:\n",
      "\n",
      "It shows a wooden floor with a white baseboard. There's a mirror leaning against the wall, reflecting part of the floor and the baseboard. The reflection seems to match the shape and outline of the floor and baseboard. The lines are straight, and the angles appear consistent.\n",
      "\n",
      "Wait, but the reflection is not perfectly straight. It seems a bit distorted, maybe due to the angle of the mirror or some imperfection in the mirror surface. So, it's not a perfect 10, but it's close.\n",
      "\n",
      "I need to decide how much this distortion affects the overall accuracy. If I consider that the distortion is minor and mostly affects the straightness of the lines slightly, I might give it an 8 or 9.\n",
      "\n",
      "But perhaps I should be stricter since the outline isn't perfectly matching. Let's see the next image and compare.\n",
      "\n",
      "Second image:\n",
      "\n",
      "This one shows a close-up of a wooden surface with a mirror embedded in it. The mirror is rectangular, and its reflection shows the opposite side of the wooden surface.\n",
      "\n",
      "Looking at the reflection, it seems to match the shape and outline of the wooden surface pretty well. The edges are clear, and the corners are sharp.\n",
      "\n",
      "However, there might be a slight misalignment in the reflection, but it's hard to tell from this angle. Given that it's a close-up, any minor distortion might be more noticeable.\n",
      "\n",
      "Comparing to the first image, this reflection seems more accurate in terms of shape and outline.\n",
      "\n",
      "I need to decide between a 9 and a 10. If I see almost no flaws, maybe it's a 10.\n",
      "\n",
      "But wait, is there any imperfection? Looking closely, the edges in the reflection seem slightly softer or less defined than the actual edges. That might be due to the camera's focus or the mirror's quality.\n",
      "\n",
      "Given that, I might lean towards a 9, acknowledging that it's very close to perfect but not quite there.\n",
      "\n",
      "Moving on to the third image:\n",
      "\n",
      "This image shows a concrete floor with some debris or marks on it. There's a mirror on the floor, reflecting the ceiling and some other elements.\n",
      "\n",
      "Focusing on the shape and outline, the reflection seems to capture the general form of the ceiling and any objects above.\n",
      "\n",
      "However, the reflection appears a bit distorted, possibly due to the mirror's placement or curvature.\n",
      "\n",
      "Comparing to the previous images, this reflection isn't as accurate in terms of shape and outline. The lines aren't as straight, and the shapes seem somewhat warped.\n",
      "\n",
      "I need to rate this accordingly. Maybe a 6 or 7, indicating that there are significant differences in the outline.\n",
      "\n",
      "But perhaps that's too harsh. Let's see.\n",
      "\n",
      "Fourth image:\n",
      "\n",
      "This image shows a wooden surface with a mirror embedded in it, similar to the second image.\n",
      "\n",
      "The reflection appears clear, with well-defined edges and corners. It seems to match the shape and outline of the wooden surface accurately.\n",
      "\n",
      "Comparing to the second image, this one looks even sharper and more precise. Maybe a 10?\n",
      "\n",
      "But again, I need to be consistent with my criteria.\n",
      "\n",
      "Fifth image:\n",
      "\n",
      "This image shows a wooden floor with a baseboard, similar to the first image, but from a different angle.\n",
      "\n",
      "The mirror is placed against the wall, reflecting part of the floor and the baseboard.\n",
      "\n",
      "The reflection seems accurate in terms of shape and outline, with straight lines and proper angles.\n",
      "\n",
      "I think this is another high scorer, perhaps a 9 or 10.\n",
      "\n",
      "Sixth image:\n",
      "\n",
      "This one shows a wooden surface with a mirror embedded in it, similar to the second and fourth images.\n",
      "\n",
      "The reflection appears slightly blurred or less defined compared to the previous ones.\n",
      "\n",
      "This might indicate some issue with the mirror's surface or the camera's focus.\n",
      "\n",
      "Given that, the shape and outline aren't as clear, making it less accurate.\n",
      "\n",
      "I might rate this around a 7 or 8.\n",
      "\n",
      "Seventh image:\n",
      "\n",
      "This image shows a concrete floor with a mirror on it, reflecting the ceiling and some objects.\n",
      "\n",
      "The reflection seems distorted, with wavy lines and misshapen forms.\n",
      "\n",
      "This indicates that the mirror might be warped or not perfectly flat, affecting the accuracy of the reflection.\n",
      "\n",
      "Comparing to the third image, this seems even more distorted.\n",
      "\n",
      "I might rate this around a 5 or 6.\n",
      "\n",
      "Eighth image:\n",
      "\n",
      "This image shows a wooden surface with a mirror embedded in it, similar to previous images.\n",
      "\n",
      "The reflection appears accurate, with clear edges and proper shapes.\n",
      "\n",
      "It seems to match the object's shape and outline very well.\n",
      "\n",
      "This is likely another high scorer, perhaps a 9 or 10.\n",
      "\n",
      "Ninth image:\n",
      "\n",
      "This image shows a similar setup, with a wooden surface and a mirror.\n",
      "\n",
      "However, the reflection appears distorted, with curved lines instead of straight ones.\n",
      "\n",
      "This suggests that the mirror isn't flat or there's some distortion in its surface.\n",
      "\n",
      "Given that, the shape and outline don't match the object accurately.\n",
      "\n",
      "I might rate this around a 6 or 7.\n",
      "\n",
      "Tenth image:\n",
      "\n",
      "This final image shows a wooden surface with a mirror embedded in it, similar to the others.\n",
      "\n",
      "The reflection appears to have some distortion, similar to the ninth image.\n",
      "\n",
      "The shapes aren't perfectly straight, indicating some issue with the mirror's surface.\n",
      "\n",
      "I might rate this similarly to the ninth image, around a 6 or 7.\n",
      "\n",
      "Now, I need to summarize my evaluations for each image, providing a score and a brief reason for that score.\n",
      "\n",
      "Let's compile that.\n",
      "\n",
      "Image 1:\n",
      "\n",
      "Score: 9\n",
      "\n",
      "Reason: Minor distortion in the reflection of the floor and baseboard, but overall shape and outline are well-preserved.\n",
      "\n",
      "Image 2:\n",
      "\n",
      "Score: 9\n",
      "\n",
      "Reason: Reflection matches the wooden surface's shape and outline very closely, with slight softness in edges.\n",
      "\n",
      "Image 3:\n",
      "\n",
      "Score: 7\n",
      "\n",
      "Reason: Reflection shows some distortion, affecting the accuracy of the shape and outline.\n",
      "\n",
      "Image 4:\n",
      "\n",
      "Score: 10\n",
      "\n",
      "Reason: Reflection is exceptionally clear and accurately matches the shape and outline of the wooden surface.\n",
      "\n",
      "Image 5:\n",
      "\n",
      "Score: 9\n",
      "\n",
      "Reason: Reflection accurately captures the floor and baseboard's shape and outline with minor imperfections.\n",
      "\n",
      "Image 6:\n",
      "\n",
      "Score: 8\n",
      "\n",
      "Reason: Reflection is slightly blurred, reducing the clarity of the shape and outline.\n",
      "\n",
      "Image 7:\n",
      "\n",
      "Score: 6\n",
      "\n",
      "Reason: Significant distortion in the reflection, leading to inaccuracies in shape and outline.\n",
      "\n",
      "Image 8:\n",
      "\n",
      "Score: 10\n",
      "\n",
      "Reason: Reflection is clear and precisely matches the shape and outline of the wooden surface.\n",
      "\n",
      "Image 9:\n",
      "\n",
      "Score: 7\n",
      "\n",
      "Reason: Distortion in the mirror causes curved lines instead of straight ones, affecting the accuracy of the reflection.\n",
      "\n",
      "Image 10:\n",
      "\n",
      "Score: 7\n",
      "\n",
      "Reason: Similar distortion as in image 9, leading to inaccuracies in the shape and outline.\n",
      "\n",
      "I need to make sure that I'm being consistent with my scoring. For example, I gave image 1 and 5 a 9, while image 4 and 8 a 10. The difference is that in images 4 and 8, the reflections are exceptionally clear and precise, whereas in images 1 and 5, there are minor distortions or softness in the edges.\n",
      "\n",
      "Similarly, images 6, 9, and 10 have more noticeable issues, leading to lower scores.\n",
      "\n",
      "I should also consider if there are any images where the reflection is completely inaccurate or missing, which would warrant a score of 0 or 1. However, from what I've seen, all images have some reflection, albeit with varying degrees of accuracy.\n",
      "\n",
      "In conclusion, based on the shape and outline matching between the object and its reflection, I've assigned scores ranging from 6 to 10, with 10 being the highest for perfect matches and 6 for reflections with significant distortions.\n",
      "\n",
      "**Final Output**\n",
      "\n",
      "To summarize, the scores for each image are as follows:\n",
      "\n",
      "1. 9 - Minor distortion, but overall shape and outline are well-preserved.\n",
      "\n",
      "2. 9 - Reflection matches closely with slight softness in edges.\n",
      "\n",
      "3. 7 - Some distortion affecting accuracy.\n",
      "\n",
      "4. 10 - Exceptionally clear and accurate reflection.\n",
      "\n",
      "5. 9 - Accurate capture with minor imperfections.\n",
      "\n",
      "6. 8 - Slight blurriness reduces clarity.\n",
      "\n",
      "7. 6 - Significant distortion leading to inaccuracies.\n",
      "\n",
      "8. 10 - Clear and precise reflection.\n",
      "\n",
      "9. 7 - Distortion causes curved lines instead of straight ones.\n",
      "\n",
      "10. 7 - Similar distortion affecting accuracy.\n",
      "\n",
      "These scores reflect the level of accuracy in the reflection's representation of the object's shape and outline, disregarding color and texture differences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 4/4 [19:28<00:00, 292.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final image scores: [0, 10, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# 配置参数\n",
    "model_name = \"QVQ-72B-Preview\"\n",
    "local_path = f\"/cpfs04/shared/CausaLLMs/HuggingfaceModels/{model_name}\"\n",
    "image_size = 64\n",
    "batch_size = 8  # 设定批处理大小，可根据显存调整\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 加载模型\n",
    "if model_name == \"Qwen2.5-VL-72B-Instruct\" or model_name == \"Qwen2.5-VL-7B-Instruct\":\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        local_path, torch_dtype=\"auto\", device_map=\"auto\"\n",
    "    )\n",
    "elif model_name == \"QVQ-72B-Preview\":\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        local_path, torch_dtype=\"auto\", device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(local_path)\n",
    "\n",
    "\n",
    "# 自定义 Dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, image_size):\n",
    "        self.image_paths = image_paths\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        return image_path\n",
    "\n",
    "\n",
    "# 处理批量图片并评分\n",
    "def evaluate_images_in_batch(image_paths):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path, \"resized_height\": image_size, \"resized_width\": image_size}\n",
    "                for image_path in image_paths\n",
    "            ] + [\n",
    "                {\"type\": \"text\", \"text\": (\n",
    "                    f\"You are an expert in visual anomaly detection, focusing primarily on the shape and outline of objects. \"\n",
    "                    f\"You will see {batch_size} images of an object in front of a mirror and its reflection. Specifically, please pay close attention to the outline and shape differences between the object in front of the mirror and its reflection, and consider whether they could reasonably be the front and back of the same object. \"\n",
    "                    \"Please rate each image individually on a scale from 0 (inconsistent) to 10 (completely harmonious and consistent). The specific scoring criteria are as follows:\"\n",
    "                    \"Score 10: Perfect match in shape and outline, considering the mirror flip.\"  \n",
    "                    \"Score 9: Minor distortions due to perspective, but overall shape is recognizable.\"  \n",
    "                    \"Score 8: Slight inconsistencies in shape, possibly due to object positioning or minor defects.\"  \n",
    "                    \"Score 7: Moderate inconsistencies, where the general shape is similar but there are noticeable differences.\"  \n",
    "                    \"Score 6: Significant inconsistencies, making it hard to recognize the reflection as the object’s reflection.\"  \n",
    "                    \"Score 5: Equal parts consistent and inconsistent, borderline case.\"  \n",
    "                    \"Score 4: Reflection shows a completely different shape, but some elements are similar.\"  \n",
    "                    \"Score 3: Reflection bears little resemblance to the object.\"  \n",
    "                    \"Score 2: Only a few parts of the reflection match the object.\"  \n",
    "                    \"Score 1: Almost no similarity in shape and outline.\"  \n",
    "                    \"Score 0: Reflection is completely different, no similarity at all.\"\n",
    "                    \"Please think this step by step and answer in the the format: The score is {your_score}. The reason is {your_reason}.\"\n",
    "                )}\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # 预处理输入\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # 生成输出\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=9000)\n",
    "\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_texts = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    # 解析评分\n",
    "    scores = []\n",
    "    for output_text in output_texts:\n",
    "        score = extract_score_after_final_answer(output_text)\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def extract_score_after_final_answer(output_text):\n",
    "    \"\"\" 从模型输出文本中提取最终评分 \"\"\"\n",
    "    pattern = r\"\\*\\*Final Answer\\*\\*.*?(\\d+)\"\n",
    "    match = re.search(pattern, output_text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        raise ValueError(f\"No number found after **Final Answer** in: {output_text}\")\n",
    "\n",
    "\n",
    "# 读取图片路径\n",
    "image_dirs = [\n",
    "    \"/cpfs04/user/hanyujin/rule-gen/datasets/mirrors/left\",\n",
    "    \"/cpfs04/user/hanyujin/rule-gen/experiments/samples/AE-Diff-16-mirror-SiT-B-1-linear-vae0040000.pt-size-64cfg-4.0-seed-0\",\n",
    "    \"/cpfs04/user/hanyujin/rule-gen/experiments/samples/AE-JEPA-Diff-16-mirror-SiT-B-1-linear-vae0040000.pt-size-64cfg-4.0-seed-0\"\n",
    "]\n",
    "\n",
    "image_paths = []\n",
    "for image_dir in image_dirs:\n",
    "    image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".png\")]\n",
    "    image_paths.extend(random.sample(image_files, min(10, len(image_files))))  # 采样10张图片\n",
    "\n",
    "# 创建 DataLoader\n",
    "dataset = ImageDataset(image_paths, image_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# 批量评分\n",
    "scores = []\n",
    "for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
    "    try:\n",
    "        batch_scores = evaluate_images_in_batch(batch)\n",
    "        scores.extend(batch_scores)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "\n",
    "# 输出最终评分\n",
    "print(\"Final image scores:\", scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yjenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
